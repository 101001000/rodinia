#!/usr/bin/env julia

using DataFrames
using Compat

const suites = ["cuda", "julia_cuda"]   # which benchmark suites to profile and compare
const baseline = "cuda"
const non_baseline = filter(suite->suite!=baseline, suites)
const root = dirname(@__DIR__)

# configuration
const ALWAYS_RUN = false    # always run the profile script, even if we have previous results
const APPEND_DATA = false   # if ALWAYS_RUN==true but we already had data, combine both sets

# NOTE: because of how we calculate totals (per-benchmark totals based on time x iterations,
#       per-suite benchmarks based on flat performance difference) it is possible to gather
#       more data for individual benchmarks, but not for individual kernels (as that would
#       skew the per-benchmark totals)


## input

info("Gathering data")

# find benchmarks common to all suites
benchmarks = Dict()
for suite in suites
    entries = readdir(joinpath(root, suite))
    benchmarks[suite] = filter(entry->(isdir(joinpath(root,suite,entry)) &&
                                       isfile(joinpath(root,suite,entry,"profile"))), entries)
end
common_benchmarks = intersect(values(benchmarks)...)

# gather profiling data
data = DataFrame(suite=String[], benchmark=String[], kernel=String[], time=Float64[])
for suite in suites, benchmark in common_benchmarks
    dir = joinpath(root, suite, benchmark)
    cd(dir) do
        # get data
        cache = joinpath(dir, "profile.csv")
        csv = Nullable{String}()
        if isfile(cache)
            csv = Nullable(readstring(cache))
        end

        if ALWAYS_RUN || isnull(csv)
            # run and collect CSV output
            csv_lines = Vector{String}()
            tag = "$benchmark,"
            info("Running $suite/$benchmark")
            cmd = pipeline(ignorestatus(`./profile`))
            for line in eachline(cmd)
                if startswith(line, tag)
                    push!(csv_lines, line)
                end
            end
            if success(cmd) && length(csv_lines) > 0
                csv_data = join(csv_lines, "\n")
                if APPEND_DATA && !isnull(csv)
                    csv_data = csv_data * "\n" * get(csv)
                end
                write(cache, csv_data)
                csv = Nullable(csv_data)
            elseif !success(cmd)
                warn("Benchmark did not succeed")
            else
                warn("Benchmark did not output any data")
            end
        end

        if !isnull(csv)
            io = IOBuffer(get(csv))
            local_data = readtable(io, header=false)
            names!(local_data, [:benchmark, :kernel, :time])
            local_data[:suite] = suite
            # create new table with a :suite column prepended
            local_data = DataFrame(suite=local_data[:suite],
                                   benchmark=local_data[:benchmark],
                                   kernel=local_data[:kernel],
                                   time=local_data[:time])
            append!(data, local_data)
        end
    end
end


## analysis

info("Processing...")

# create a summary with a column per suite (except the baseline)
summary = DataFrame(benchmark=String[], kernel=String[])
for suite in non_baseline
    summary[Symbol(suite)] = Float64[]
end

# add time totals for each benchmark
# NOTE: we do this before summarizing across iterations, to make totals more fair
#       (ie. the totals are affected by the amount of iterations for each kernel)
append!(data, by(data, [:suite, :benchmark],
                 dt->DataFrame(kernel="total", time=sum(dt[:time]))))

# summarize across iterations
grouped_data = by(data, [:suite, :benchmark, :kernel],
                  dt->DataFrame(time=minimum(dt[:time]))
                 )

# calculate the slowdown/improvement compared against the baseline
for benchmark in unique(grouped_data[:benchmark])
    # get the data for this benchmark
    benchmark_data = grouped_data[grouped_data[:benchmark] .== benchmark, :]
    for kernel in unique(benchmark_data[:kernel])
        # get the data for this kernel
        kernel_data = benchmark_data[benchmark_data[:kernel] .== kernel, :]
        if sort(kernel_data[:suite]) != sort(suites)
            warn("- kernel $kernel: don't have data for all suites")
            continue
        end

        # compare other suites against the chosen baseline
        baseline_data = kernel_data[kernel_data[:suite] .== baseline, :]
        others_data = kernel_data[kernel_data[:suite] .!= baseline, :]
        for suite in others_data[:suite]
            suite_data = kernel_data[kernel_data[:suite] .== suite, :]
            difference = suite_data[:time][1] / baseline_data[:time][1]
            push!(summary, [benchmark kernel difference])
        end
    end
end

# add difference totals for each suite (based on previous totals)
# NOTE: this total only looks at each benchmark's performance increase/loss,
#       not only ignores the iteration count, but the execution time altogether
# FIXME: can't we do this with a `by`, summarizing over all remaining columns?
totals = []
for suite in names(summary)[3:end]
    push!(totals, mean(summary[summary[:kernel] .== "total", suite]))
end
push!(summary, ["total", "total", totals...])

# tools for accessing stats
suite_stats(suite) = summary[summary[:kernel] .== "total", [:benchmark, Symbol(suite)]]
benchmark_stats(benchmark) = summary[summary[:benchmark] .== benchmark, :]
println(suite_stats("julia_cuda"))
