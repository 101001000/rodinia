#!/usr/bin/env julia

using DataFrames
using Compat
using Glob

const suites = ["cuda", "julia_cuda"]   # which benchmark suites to profile and compare
const baseline = "cuda"
const non_baseline = filter(suite->suite!=baseline, suites)
const root = dirname(@__DIR__)

# configuration
const ALWAYS_RUN =  parse(Bool, get(ENV, "ALWAYS_RUN",  "false"))   # always run the profile script, even if we have previous results
const APPEND_DATA = parse(Bool, get(ENV, "APPEND_DATA", "false"))   # if ALWAYS_RUN==true but we already had data, combine both sets

# NOTE: because of how we calculate totals (per-benchmark totals based on time x iterations,
#       per-suite benchmarks based on flat performance difference) it is possible to gather
#       more data for individual benchmarks, but not for individual kernels (as that would
#       skew the per-benchmark totals)


## input

info("Gathering data")

# find benchmarks common to all suites
benchmarks = Dict()
for suite in suites
    entries = readdir(joinpath(root, suite))
    benchmarks[suite] = filter(entry->(isdir(joinpath(root,suite,entry)) &&
                                       isfile(joinpath(root,suite,entry,"profile"))), entries)
end
common_benchmarks = intersect(values(benchmarks)...)

# gather profiling data
data = DataFrame(suite=String[], benchmark=String[], kernel=String[], time=Float64[])
for suite in suites, benchmark in common_benchmarks
    dir = joinpath(root, suite, benchmark)
    cd(dir) do
        cache = joinpath(dir, "profile.csv")

        if ALWAYS_RUN || !isfile(cache)
            glob_pattern = "profile.csv*"

            # Delete old output files.
            for output_file in glob(glob_pattern, dir)
                rm(output_file)
            end

            # Run and collect CSV output.
            info("Running $suite/$benchmark")

            cmd_success = success(ignorestatus(`nvprof --profile-from-start off --profile-child-processes --print-gpu-trace --normalized-time-unit us --csv --log-file $cache.%p ./profile --depwarn=no`))
            output_files = glob(glob_pattern, dir)

            if !cmd_success
                warn("$suite/$benchmark did not succeed")
            elseif length(output_files) == 0
                warn("no output files for $suite/$benchmark")
            elseif length(output_files) > 1
                warn("too many output files for $suite/$benchmark")
            else
                mv(output_files[1], cache, remove_destination = true)
            end
        end

        if isfile(cache)
            local_data = readtable(cache, header = false, skipstart = 5)

            if size(local_data, 1) != 0
                # Delete all CUDA rows. FIXME: find a better way
                deleterows!(local_data, find(isna(local_data[:x3])))
                # Append suite and benchmark columns.
                local_data[:suite] = suite
                local_data[:benchmark] = benchmark

                # Extract the kernel names.
                kernels = local_data[:x17]

                for i = 1:length(kernels)

                    jl_match = match(r"julia_(.*)_[0-9]+ .*", kernels[i])

                    if jl_match != nothing

                        kernels[i] = jl_match.captures[1]
                        continue
                    end

                    cu_match = match(r"(.*)\(.*", kernels[i])

                    if cu_match != nothing

                        kernels[i] = cu_match.captures[1]
                        continue
                    end
                end

                local_data = DataFrame(suite = local_data[:suite],
                                       benchmark = local_data[:benchmark],
                                       kernel = kernels,
                                       time = local_data[:x2])
                append!(data, local_data)
            else
                warn("no data for $suite/$benchmark")
            end
        end
    end
end


## analysis

info("Processing...")

# create a summary with a column per suite (except the baseline)
summary = DataFrame(benchmark=String[], kernel=String[])
for suite in non_baseline
    summary[Symbol(suite)] = Float64[]
end

# add time totals for each benchmark
# NOTE: we do this before summarizing across iterations, to make totals more fair
#       (ie. the totals are affected by the amount of iterations for each kernel)
# NOTE: we must normalize these timings by the number of iterations,
#       as not every suite might have executed the same number of times
append!(data, by(data, [:suite, :benchmark],
                 dt->DataFrame(kernel="total",
                               time=sum(dt[:time])/size(dt, 1))))

# summarize across iterations
grouped_data = by(data, [:suite, :benchmark, :kernel],
                  dt->DataFrame(time=minimum(dt[:time]))
                 )

# calculate the slowdown/improvement compared against the baseline
for benchmark in unique(grouped_data[:benchmark])
    # get the data for this benchmark
    benchmark_data = grouped_data[grouped_data[:benchmark] .== benchmark, :]
    for kernel in unique(benchmark_data[:kernel])
        # get the data for this kernel
        kernel_data = benchmark_data[benchmark_data[:kernel] .== kernel, :]
        if sort(kernel_data[:suite]) != sort(suites)
            warn("$benchmark - $kernel: don't have data for all suites")
            continue
        end

        # compare other suites against the chosen baseline
        baseline_data = kernel_data[kernel_data[:suite] .== baseline, :]
        others_data = kernel_data[kernel_data[:suite] .!= baseline, :]
        for suite in others_data[:suite]
            suite_data = kernel_data[kernel_data[:suite] .== suite, :]
            difference = suite_data[:time][1] / baseline_data[:time][1]
            push!(summary, [benchmark kernel difference])
        end
    end
end

# add difference totals for each suite (based on previous totals)
# NOTE: this total only looks at each benchmark's performance increase/loss,
#       not only ignores the iteration count, but the execution time altogether
# FIXME: can't we do this with a `by`, summarizing over all remaining columns?
totals = []
for suite in names(summary)[3:end]
    push!(totals, mean(summary[summary[:kernel] .== "total", suite]))
end
push!(summary, ["total", "total", totals...])

# tools for accessing stats
suite_stats(suite) = summary[summary[:kernel] .== "total", [:benchmark, Symbol(suite)]]
benchmark_stats(benchmark) = summary[summary[:benchmark] .== benchmark, :]
println(suite_stats("julia_cuda"))
