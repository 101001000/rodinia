#!/usr/bin/env julia

using DataFrames
using Compat
using Glob

const suites = ["cuda", "julia_cuda"]   # which benchmark suites to profile and compare
const baseline = "cuda"
const non_baseline = filter(suite->suite!=baseline, suites)
const root = dirname(@__DIR__)

# configuration
const MIN_KERNEL_ITERATIONS = 10
const MAX_KERNEL_UNCERTAINTY = 0.02
const MAX_BENCHMARK_RUNS = 100
const MAX_BENCHMARK_SECONDS = 300

# NOTE: because of how we calculate totals (per-benchmark totals based on time x iterations,
#       per-suite benchmarks based on flat performance difference) it is possible to gather
#       more data for individual benchmarks, but not for individual kernels (as that would
#       skew the per-benchmark totals)


## input

info("Gathering data")

# find benchmarks common to all suites
benchmarks = Dict()
for suite in suites
    entries = readdir(joinpath(root, suite))
    benchmarks[suite] = filter(entry->(isdir(joinpath(root,suite,entry)) &&
                                       isfile(joinpath(root,suite,entry,"profile"))), entries)
end
common_benchmarks = intersect(values(benchmarks)...)

# run a benchmark once, returning the measurement data
function run_benchmark(dir, suite, benchmark)
    output_file = "nvprof.csv"
    output_pattern = "nvprof.csv.*"

    # delete old profile output
    rm.(glob(output_pattern, dir))

    # run and measure
    out = Pipe()
    cmd = ```
        nvprof
        --profile-from-start off
        --profile-child-processes
        --unified-memory-profiling off
        --print-gpu-trace
        --normalized-time-unit us
        --csv
        --log-file $output_file.%p
        ./profile --depwarn=no
    ```
    cmd_success = cd(dir) do
        success(pipeline(ignorestatus(cmd), stdout=out, stderr=out))
    end
    close(out.in)
    output_files = glob(output_pattern, dir)

    # process data
    if !cmd_success
        println(readstring(out))
        error("benchmark did not succeed")
    else
        output_data = []
        for output_file in output_files
            try
                push!(output_data, read_data(output_file, suite, benchmark))
            catch
                nothing
            end
        end
        if length(output_data) == 0
            error("no output files")
        elseif length(output_data) > 1
            error("too many output files")
        else
            return output_data[1]
        end
    end
end

function read_data(output_path, suite, benchmark)
    # nuke the headers and read the data
    output = readlines(output_path; chomp=false)
    raw_data = mktemp() do path,io
        write(io, output[4])
        write(io, output[6:end])
        flush(io)
        return readtable(path)
    end
    size(raw_data, 1) == 0 && error("no data")

    # remove API calls
    raw_data = raw_data[!startswith.(raw_data[:Name], "[CUDA"), :]

    # demangle kernel names
    kernels = raw_data[:Name]
    for i = 1:length(kernels)
        jl_match = match(r"ptxcall_(.*)_[0-9]+ .*", kernels[i])
        if jl_match != nothing
            kernels[i] = jl_match.captures[1]
            continue
        end

        cu_match = match(r"(.*)\(.*", kernels[i])
        if cu_match != nothing
            kernels[i] = cu_match.captures[1]
            continue
        end
    end

    # generate a nicer table
    rows = size(raw_data, 1)
    data = DataFrame(suite = repeat([suite]; inner=rows),   # DataFramesMeta.jl/#46
                     benchmark = benchmark,
                     kernel = kernels,
                     time = raw_data[:Duration])

    # these benchmarks spend a variable amount of time per kernel, so we can't aggregate.
    # work around this by giving them unique names
    blacklist = Dict(
        "bfs"             => ["Kernel", "Kernel2"],
        "leukocyte"       => ["IMGVF_kernel"],
        "lud"             => ["lud_perimeter", "lud_internal"],
        "particlefilter"  => ["find_index_kernel"],
        "nw"              => ["needle_cuda_shared_1", "needle_cuda_shared_2"],
    )
    if haskey(blacklist, benchmark)
        kernels = blacklist[benchmark]
        for kernel in kernels
            for i in 1:size(data, 1)
                if data[:kernel][i] in kernels
                    data[:kernel][i] *= "_$i"
                end
            end
        end
    end

    return data
end

# check if measurements are accurate enough
function is_accurate(data)
    # group across iterations
    grouped = by(data, [:suite, :benchmark, :kernel],
                 dt->DataFrame(iterations=length(dt[:time]),
                               abs_uncert=std(dt[:time]),     # TODO: lognormal
                               best=minimum(dt[:time]))
                )

    # calculate relative uncertainty
    grouped[:rel_uncert] = grouped[:abs_uncert] ./ abs(grouped[:best])

    return all(i->i>=MIN_KERNEL_ITERATIONS, grouped[:iterations]) &&
           all(val->val<MAX_KERNEL_UNCERTAINTY, grouped[:rel_uncert])
end

# gather profiling data
data = DataFrame(suite=String[], benchmark=String[], kernel=String[], time=Float64[])
for suite in suites, benchmark in common_benchmarks
    info("Processing $suite/$benchmark")
    dir = joinpath(root, suite, benchmark)
    cache_path = joinpath(dir, "profile.csv")

    if isfile(cache_path)
        local_data = readtable(cache_path)
    else
        t0 = time()

        # iteration 0
        iter = 1
        local_data = run_benchmark(dir, suite, benchmark)

        # additional iterations
        while (time()-t0) < MAX_BENCHMARK_SECONDS &&
            iter < MAX_BENCHMARK_RUNS &&
            !is_accurate(local_data)
            iter += 1
            append!(local_data, run_benchmark(dir, suite, benchmark))
        end

        writetable(cache_path, local_data)
    end

    append!(data, local_data)
end


## analysis

info("Processing...")

# create a summary with a column per suite (except the baseline)
summary = DataFrame(benchmark=String[], kernel=String[])
for suite in non_baseline
    summary[Symbol(suite)] = Float64[]
end

# add time totals for each benchmark
# NOTE: we do this before summarizing across iterations, to make totals more fair
#       (ie. the totals are affected by the amount of iterations for each kernel)
# NOTE: we must normalize these timings by the number of iterations,
#       as not every suite might have executed the same number of times
append!(data, by(data, [:suite, :benchmark],
                 dt->DataFrame(kernel="total",
                               time=sum(dt[:time])/size(dt, 1))))

# summarize across iterations
grouped_data = by(data, [:suite, :benchmark, :kernel],
                  dt->DataFrame(time=minimum(dt[:time]))
                 )

# calculate the slowdown/improvement compared against the baseline
for benchmark in unique(grouped_data[:benchmark])
    # get the data for this benchmark
    benchmark_data = grouped_data[grouped_data[:benchmark] .== benchmark, :]
    for kernel in unique(benchmark_data[:kernel])
        # get the data for this kernel
        kernel_data = benchmark_data[benchmark_data[:kernel] .== kernel, :]
        if sort(kernel_data[:suite]) != sort(suites)
            warn("$benchmark - $kernel: don't have data for all suites")
            continue
        end

        # compare other suites against the chosen baseline
        baseline_data = kernel_data[kernel_data[:suite] .== baseline, :]
        others_data = kernel_data[kernel_data[:suite] .!= baseline, :]
        for suite in others_data[:suite]
            suite_data = kernel_data[kernel_data[:suite] .== suite, :]
            difference = suite_data[:time][1] / baseline_data[:time][1]
            push!(summary, [benchmark kernel difference])
        end
    end
end

# add difference totals for each suite (based on previous totals)
# NOTE: this total only looks at each benchmark's performance increase/loss,
#       not only ignores the iteration count, but the execution time altogether
# FIXME: can't we do this with a `by`, summarizing over all remaining columns?
totals = []
for suite in names(summary)[3:end]
    push!(totals, mean(summary[summary[:kernel] .== "total", suite]))
end
push!(summary, ["total", "total", totals...])

# tools for accessing stats
suite_stats(suite) = summary[summary[:kernel] .== "total", [:benchmark, Symbol(suite)]]
benchmark_stats(benchmark) = summary[summary[:benchmark] .== benchmark, :]
println(suite_stats("julia_cuda"))


## plotting

using Plots
pyplot()

bar(suite_stats("julia_cuda")[:benchmark],
    100.*suite_stats("julia_cuda")[:julia_cuda].-100;
    legend=false,
    rotation=45,
    xlabel="benchmark",
    ylabel = "slowdown (%)")

png("results")
